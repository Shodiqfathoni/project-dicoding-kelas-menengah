# -*- coding: utf-8 -*-
"""biomasa_time_series_rev.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A9K9m1sShIzWb6pmyDqRKQkhKbDsLc7B

# DATA DIRI
- Nama: Muhammad Shodiq Fathoni
- Email: shodiqfathoni3@gmail.com
- Id Dicoding: shodiq29

## project prediksi menggunakan dataset konsumsi energi biomassa untuk pembangkit listrik di romania
"""

import seaborn as sns
import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, Dropout

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/data project/electricityConsumptionAndProductioction.csv')
df.tail()

df.isnull().sum()

date = df['DateTime'].values
bio = df[['Biomass']].values

from sklearn.preprocessing import MinMaxScaler
min_max_scaler = MinMaxScaler()
bio = min_max_scaler.fit_transform(bio.reshape(-1, 1)).flatten()

# calculate threshold MAE
threshold_mae = (bio.max() - bio.min()) * 10/100
print(threshold_mae)

plt.figure(figsize=(15,5))
plt.plot(date, bio)
plt.title('Biomass consume',
          fontsize=20);

from sklearn.model_selection import train_test_split
date_latih, date_test, bio_latih, bio_test = train_test_split(date,bio, test_size=0.2, shuffle=False)

print(bio_latih.shape)
print(bio_test.shape)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(bio_latih, window_size=60, batch_size=100, shuffle_buffer=1000)
test_set = windowed_dataset(bio_test, window_size=60, batch_size=100, shuffle_buffer=1000)
model = tf.keras.models.Sequential([
    Bidirectional(LSTM(60, return_sequences=True)),
    Dropout(0.2),
    Bidirectional(LSTM(60)),
    Dense(30, activation="relu"),
    Dense(10, activation="relu"),
    Dense(1),
])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<0.10):
      print("\nmae telah mencapai <10%!")
      self.model.stop_training = True
callbacks = myCallback()

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set,epochs=100,validation_data=test_set,verbose=1,callbacks=[callbacks])

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))

# Loss
ax1.plot(history.history['loss'])
ax1.plot(history.history['val_loss'])
ax1.legend(['Loss','Val Loss'])
ax1.set_xlabel('Epoch', fontsize=12)
ax1.set_ylabel('Loss', fontsize=12)
ax1.set_title('Loss', fontsize=20)

# MAE
ax2.plot(history.history['mae'])
ax2.plot(history.history['val_mae'])
ax2.legend(['mae','Val mae'])
ax2.set_xlabel('Epoch', fontsize=12)
ax2.set_ylabel('Mean Absolute Error', fontsize=12)
ax2.set_title('Mean Absolute Error', fontsize=20)
plt.show()